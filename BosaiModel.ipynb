{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BosaiModel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TraverseTowner/my_dataset/blob/master/BosaiModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvSkU3Pa420G",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9mLz99TRn7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "# from keras.models import Sequential\n",
        "# from keras.callbacks import EarlyStopping\n",
        "# from keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dropout, Dense, Activation, LSTM\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras import backend as K \n",
        "\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "from math import floor\n",
        "from typing import List\n",
        "from typing import Sequence\n",
        "import time\n",
        "from tensorflow.keras.utils import to_categorical   \n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "# tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nM25ARL4Fvx",
        "colab_type": "code",
        "outputId": "a41a22eb-1926-4546-c70a-172714d54238",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1293
        }
      },
      "source": [
        "VOCAB_SIZE = 75313 + 1\n",
        "EMBEDDING_SIZE = 50\n",
        "INPUT_SIZE = 8\n",
        "OUTPUT_SIZE = 4635\n",
        "\n",
        "def __RNN():\n",
        "        model = Sequential()\n",
        "#         model.add(tf.keras.layers.Dense(5, input_shape=(5,), activation=tf.nn.relu)) \n",
        "        model.add(Embedding(VOCAB_SIZE, EMBEDDING_SIZE, input_length=INPUT_SIZE, input_shape=(VOCAB_SIZE, )))\n",
        "        model.add(SpatialDropout1D(0.2))\n",
        "        model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "        model.add(Dense(OUTPUT_SIZE, activation='softmax'))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "def __one_hot(arr: np.ndarray, num_classes: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Obtain the one-hot representation from an one-dimensional class.\n",
        "        :param a: One-dimensional array that contains the classes (e.g. [1,1,...,6])\n",
        "        :param num_classes: The total number of classes.\n",
        "        :return 2d_np_array:\n",
        "        \"\"\"\n",
        "\n",
        "        res = np.zeros((arr.size, num_classes), dtype='int')\n",
        "\n",
        "        for i, group in enumerate(arr):\n",
        "            # The group begins from 1, and hence we must subtact by one.\n",
        "            res[i][group-1] = 1\n",
        "        return res\n",
        "\n",
        "      \n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "def train_model(features_dir: str, target_dir: str, save_file_name: str):\n",
        "\n",
        "#         my_model = __RNN()\n",
        "#         # model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
        "#         print(my_model.summary())\n",
        "        \n",
        "        TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "        tf.logging.set_verbosity(tf.logging.INFO)\n",
        "        \n",
        "#         resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "#         tf.config.experimental_connect_to_host(resolver.master())\n",
        "#         tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "#         strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "        \n",
        "        \n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        tf.config.experimental_connect_to_host(resolver.master())\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "#         resolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "#         tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "#         strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "\n",
        "        TOTAL_NUMBER_OF_FILES_TO_COMPUTE = 10\n",
        "        CHUNK_SIZE = 20000\n",
        "        number_of_rows_to_skip = 0\n",
        "        \n",
        "    \n",
        "        with strategy.scope():\n",
        "\n",
        "            my_model = __RNN()\n",
        "            # model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
        "            print(my_model.summary())\n",
        "#             tpu_model = tf.contrib.tpu.keras_to_tpu_model(my_model, strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "            for file_id in range(TOTAL_NUMBER_OF_FILES_TO_COMPUTE):\n",
        "              \n",
        "              for _ in range(int(500000/CHUNK_SIZE)):\n",
        "\n",
        "                X = np.loadtxt(features_dir + '/' + str(file_id) + '.txt', delimiter=',', dtype='int', skiprows = number_of_rows_to_skip, max_rows=CHUNK_SIZE)  \n",
        "                t = np.loadtxt(target_dir + '/' + str(file_id) + '.txt', delimiter=',', dtype='int', skiprows = number_of_rows_to_skip, max_rows=CHUNK_SIZE)\n",
        "\n",
        "                number_of_rows_to_skip += CHUNK_SIZE\n",
        "\n",
        "                T = to_categorical(t, num_classes=OUTPUT_SIZE)\n",
        "\n",
        "                print(\"X.shape\", X.shape)\n",
        "                print(\"T.shape\", T.shape)\n",
        "\n",
        "                X_train, X_test, T_train, T_test = train_test_split(X, T, test_size=0.10, random_state=42)\n",
        "\n",
        "                my_model.fit(X_train,\n",
        "                            T_train,\n",
        "                            batch_size=128,\n",
        "                            epochs=5,\n",
        "                            steps_per_epoch = 140,\n",
        "                            validation_data=(X_test, T_test),\n",
        "                            callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0001)])\n",
        "\n",
        "                print(str(number_of_rows_to_skip))\n",
        "                  \n",
        "                if number_of_rows_to_skip == 500000:\n",
        "                    print(\"FINISHED PROCESSING FILE \" + str(file_id))\n",
        "                    my_model.save('my_model_weights_' + str(file_id))\n",
        "                    number_of_rows_to_skip = 0\n",
        "\n",
        "# train_model(\"my_dataset/feature_5000_X\", \"my_dataset/output_5000_t\", \"my_awesome_model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0621 15:55:33.654155 140225416955776 tpu_strategy_util.py:57] TPU system %s has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
            "I0621 15:55:33.656020 140225416955776 tpu_strategy_util.py:61] Initializing the TPU system: 10.91.167.194:8470\n",
            "I0621 15:55:41.125016 140225416955776 tpu_strategy_util.py:102] Finished initializing TPU system.\n",
            "I0621 15:55:41.127674 140225416955776 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.91.167.194:8470) for TPU system metadata.\n",
            "I0621 15:55:41.142316 140225416955776 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0621 15:55:41.144161 140225416955776 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0621 15:55:41.145426 140225416955776 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0621 15:55:41.146566 140225416955776 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0621 15:55:41.147750 140225416955776 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 18432009206338522368)\n",
            "I0621 15:55:41.149623 140225416955776 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 1044198566132582588)\n",
            "I0621 15:55:41.152492 140225416955776 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 1116105278045985036)\n",
            "I0621 15:55:41.153902 140225416955776 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5838829045038428817)\n",
            "I0621 15:55:41.156215 140225416955776 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 11100175456247752499)\n",
            "I0621 15:55:41.157674 140225416955776 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 15749071820545892020)\n",
            "I0621 15:55:41.160119 140225416955776 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 8978322474173645417)\n",
            "I0621 15:55:41.161404 140225416955776 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 12121015184128149973)\n",
            "I0621 15:55:41.162612 140225416955776 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 14630684599095211935)\n",
            "I0621 15:55:41.164852 140225416955776 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 13915999915358260456)\n",
            "I0621 15:55:41.166643 140225416955776 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16747889493625786579)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (None, 75314, 50)         3765700   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_10 (Spatia (None, 75314, 50)         0         \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 100)               60400     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 4635)              468135    \n",
            "=================================================================\n",
            "Total params: 4,294,235\n",
            "Trainable params: 4,294,235\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "X.shape (20000, 8)\n",
            "T.shape (20000, 4635)\n",
            "Epoch 1/5\n",
            "136/140 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0621 15:56:45.036142 140225416955776 training_distributed.py:262] Running validation at fit epoch: 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 15s 934ms/step\n",
            "16/16 [==============================] - 15s 934ms/step\n",
            "140/140 [==============================] - 26s 185ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "20000\n",
            "X.shape (20000, 8)\n",
            "T.shape (20000, 4635)\n",
            "Epoch 1/5\n",
            "136/140 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0621 15:58:10.077040 140225416955776 training_distributed.py:262] Running validation at fit epoch: 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 18s 1s/step\n",
            "16/16 [==============================] - 18s 1s/step\n",
            "140/140 [==============================] - 31s 219ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
            "40000\n",
            "X.shape (20000, 8)\n",
            "T.shape (20000, 4635)\n",
            "Epoch 1/5\n",
            "139/140 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0621 15:59:51.617652 140225416955776 training_distributed.py:262] Running validation at fit epoch: 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-55205f3b9c16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mnumber_of_rows_to_skip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"my_dataset/feature_5000_X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"my_dataset/output_5000_t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"my_awesome_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-55205f3b9c16>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(features_dir, target_dir, save_file_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m                             \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m140\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                             callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0001)])\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_rows_to_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     return training_arrays.fit_loop(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\u001b[0m in \u001b[0;36mexperimental_tpu_fit_loop\u001b[0;34m(model, dataset, epochs, verbose, callbacks, initial_epoch, steps_per_epoch, val_dataset, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    272\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m           callbacks=callbacks)\n\u001b[0m\u001b[1;32m    275\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\u001b[0m in \u001b[0;36mexperimental_tpu_test_loop\u001b[0;34m(model, dataset, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    315\u001b[0m   \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m   \u001b[0mcurrent_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m   \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m   scope = dist_utils.distributed_scope(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py\u001b[0m in \u001b[0;36mget_iterator\u001b[0;34m(dataset, distribution_strategy)\u001b[0m\n\u001b[1;32m    542\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m   \u001b[0minitialize_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py\u001b[0m in \u001b[0;36minitialize_iterator\u001b[0;34m(iterator, distribution_strategy)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0minit_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m       \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 941\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    942\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1164\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1165\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1342\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1343\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1333\u001b[0m                                       target_list, run_metadata)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzCLcgYyyFjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 75313 + 1\n",
        "EMBEDDING_SIZE = 50\n",
        "INPUT_SIZE = 8\n",
        "OUTPUT_SIZE = 4635\n",
        "\n",
        "def __RNN():\n",
        "        model = Sequential()\n",
        "#         model.add(tf.keras.layers.Dense(5, input_shape=(5,), activation=tf.nn.relu)) \n",
        "        model.add(Embedding(VOCAB_SIZE, EMBEDDING_SIZE, input_length=INPUT_SIZE))\n",
        "        model.add(SpatialDropout1D(0.2))\n",
        "        model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "        model.add(Dense(OUTPUT_SIZE, activation='softmax'))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "def __one_hot(arr: np.ndarray, num_classes: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Obtain the one-hot representation from an one-dimensional class.\n",
        "        :param a: One-dimensional array that contains the classes (e.g. [1,1,...,6])\n",
        "        :param num_classes: The total number of classes.\n",
        "        :return 2d_np_array:\n",
        "        \"\"\"\n",
        "\n",
        "        res = np.zeros((arr.size, num_classes), dtype='int')\n",
        "\n",
        "        for i, group in enumerate(arr):\n",
        "            # The group begins from 1, and hence we must subtact by one.\n",
        "            res[i][group-1] = 1\n",
        "        return res\n",
        "\n",
        "\n",
        "def train_model_gpu(features_dir: str, target_dir: str, save_file_name: str):\n",
        "\n",
        "        TOTAL_NUMBER_OF_FILES_TO_COMPUTE = 1\n",
        "        CHUNK_SIZE = 100000\n",
        "        number_of_rows_to_skip = 0\n",
        "        my_model = __RNN()\n",
        "\n",
        "        print(my_model.summary())\n",
        "\n",
        "        for file_id in range(TOTAL_NUMBER_OF_FILES_TO_COMPUTE):\n",
        "\n",
        "          start_time = time.time()\n",
        "          for _ in range(int(500000/CHUNK_SIZE)):\n",
        "\n",
        "            X = np.loadtxt(features_dir + '/' + str(file_id) + '.txt', delimiter=',', dtype='int', skiprows = number_of_rows_to_skip, max_rows=CHUNK_SIZE)  \n",
        "            t = np.loadtxt(target_dir + '/' + str(file_id) + '.txt', delimiter=',', dtype='int', skiprows = number_of_rows_to_skip, max_rows=CHUNK_SIZE)\n",
        "\n",
        "            number_of_rows_to_skip += CHUNK_SIZE\n",
        "\n",
        "#             T = to_categorical(t, num_classes=OUTPUT_SIZE)\n",
        "            T = __one_hot(t, num_classes=OUTPUT_SIZE)\n",
        "\n",
        "#             print(\"X.shape\", X.shape)\n",
        "#             print(\"T.shape\", T.shape)\n",
        "\n",
        "            my_model.fit(X,\n",
        "                       T,\n",
        "                        batch_size=128,\n",
        "                        epochs=5,\n",
        "                        # validation_data=(X_test, T_test),\n",
        "                        validation_split=0.1,\n",
        "                        callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0001)])\n",
        "\n",
        "            print(\"Number of Rows Parsed:\", str(number_of_rows_to_skip))\n",
        "            elapsed_time = time.time() - start_time\n",
        "              \n",
        "            if number_of_rows_to_skip == 500000:\n",
        "                print(\"Finished Processing File:\" + str(file_id))\n",
        "                print(\"ðŸŽ‰ Took\", floor(elapsed_time / 60), \"minutes\", round(elapsed_time % 60), \"seconds\")\n",
        "                my_model.save('my_model_weights_' + str(file_id))\n",
        "                number_of_rows_to_skip = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZENy0P9HyGzQ",
        "colab_type": "code",
        "outputId": "69d15907-dc9a-4d2a-ff18-f58134ffe5a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "train_model_gpu(\"my_dataset/feature_5000_X\", \"my_dataset/output_5000_t\", \"my_awesome_model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 8, 50)             3765700   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 8, 50)             0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               60400     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4635)              468135    \n",
            "=================================================================\n",
            "Total params: 4,294,235\n",
            "Trainable params: 4,294,235\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "90000/90000 [==============================] - 24s 263us/sample - loss: 3.1382 - acc: 0.0552 - val_loss: 15.4701 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "90000/90000 [==============================] - 23s 259us/sample - loss: 2.8995 - acc: 0.0549 - val_loss: 16.3806 - val_acc: 0.0000e+00\n",
            "Number of Rows Parsed: 100000\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "90000/90000 [==============================] - 23s 252us/sample - loss: 3.1300 - acc: 0.0541 - val_loss: 18.5325 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "90000/90000 [==============================] - 23s 253us/sample - loss: 2.8849 - acc: 0.0631 - val_loss: 17.4605 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "90000/90000 [==============================] - 23s 254us/sample - loss: 2.6510 - acc: 0.1302 - val_loss: 16.8264 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "90000/90000 [==============================] - 23s 253us/sample - loss: 2.2880 - acc: 0.2712 - val_loss: 18.2837 - val_acc: 0.0000e+00\n",
            "Number of Rows Parsed: 200000\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "90000/90000 [==============================] - 20s 219us/sample - loss: 2.9676 - acc: 0.1234 - val_loss: 16.7395 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "90000/90000 [==============================] - 20s 220us/sample - loss: 2.4835 - acc: 0.2292 - val_loss: 17.0546 - val_acc: 0.0000e+00\n",
            "Number of Rows Parsed: 300000\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "90000/90000 [==============================] - 23s 253us/sample - loss: 2.8423 - acc: 0.1692 - val_loss: 17.0103 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "90000/90000 [==============================] - 23s 253us/sample - loss: 2.3101 - acc: 0.2877 - val_loss: 17.0704 - val_acc: 0.0000e+00\n",
            "Number of Rows Parsed: 400000\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "90000/90000 [==============================] - 20s 219us/sample - loss: 2.8044 - acc: 0.2000 - val_loss: 16.5901 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "90000/90000 [==============================] - 20s 217us/sample - loss: 2.2414 - acc: 0.3290 - val_loss: 16.9289 - val_acc: 0.0000e+00\n",
            "Number of Rows Parsed: 500000\n",
            "Finished Processing File:0\n",
            "ðŸŽ‰ Took 4 minutes 43 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2TbiBjIyHHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqXjmJ2eyHOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def push_to_git():\n",
        "  !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNGWeJyayHSf",
        "colab_type": "code",
        "outputId": "7954168b-1a39-43a1-8ef4-8e3724913673",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "push_to_git()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_dataset  my_model_weights_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGrloxfifQfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_git():\n",
        "  !git init\n",
        "  !git config --global user.email \"isshinada@gmail.com\"\n",
        "  !git remote add origin https://github.com/TraverseTowner/my_dataset.git\n",
        "#   !git remote set-url origin git@github.com:TraverseTowner/my_dataset.git\n",
        "def lol_cake(file_name):\n",
        "  !git add $file_name\n",
        "  !git commit -m \"msg\"\n",
        "  !git push origin master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "127sa37hDH_E",
        "colab_type": "code",
        "outputId": "8018604b-9349-4bc5-c994-033078129de0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1431
        }
      },
      "source": [
        "init_git()\n",
        "lol_cake(\"my_model_weights_0\")\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpenSSH_7.6p1 Ubuntu-4ubuntu0.3, OpenSSL 1.0.2n  7 Dec 2017\r\n",
            "debug1: Reading configuration data /etc/ssh/ssh_config\r\n",
            "debug1: /etc/ssh/ssh_config line 19: Applying options for *\r\n",
            "debug2: resolving \"github.com\" port 22\r\n",
            "debug2: ssh_connect_direct: needpriv 0\r\n",
            "debug1: Connecting to github.com [13.229.188.59] port 22.\r\n",
            "debug1: Connection established.\r\n",
            "debug1: permanently_set_uid: 0/0\r\n",
            "debug1: key_load_public: No such file or directory\r\n",
            "debug1: identity file /root/.ssh/id_rsa type -1\r\n",
            "debug1: key_load_public: No such file or directory\r\n",
            "debug1: identity file /root/.ssh/id_rsa-cert type -1\r\n",
            "debug1: key_load_public: No such file or directory\r\n",
            "debug1: identity file /root/.ssh/id_dsa type -1\r\n",
            "debug1: key_load_public: No such file or directory\r\n",
            "debug1: identity file /root/.ssh/id_dsa-cert type -1\r\n",
            "debug1: key_load_public: No such file or directory\r\n",
            "debug1: identity file /root/.ssh/id_ecdsa type -1\r\n",
            "debug1: key_load_public: No such file or directory\r\n",
            "debug1: identity file /root/.ssh/id_ecdsa-cert type -1\r\n",
            "debug1: key_load_public: No such file or directory\r\n",
            "debug1: identity file /root/.ssh/id_ed25519 type -1\r\n",
            "debug1: key_load_public: No such file or directory\r\n",
            "debug1: identity file /root/.ssh/id_ed25519-cert type -1\r\n",
            "debug1: Local version string SSH-2.0-OpenSSH_7.6p1 Ubuntu-4ubuntu0.3\r\n",
            "debug1: Remote protocol version 2.0, remote software version babeld-f3847d63\r\n",
            "debug1: no match: babeld-f3847d63\r\n",
            "debug2: fd 3 setting O_NONBLOCK\r\n",
            "debug1: Authenticating to github.com:22 as 'git'\r\n",
            "debug3: send packet: type 20\r\n",
            "debug1: SSH2_MSG_KEXINIT sent\r\n",
            "debug3: receive packet: type 20\r\n",
            "debug1: SSH2_MSG_KEXINIT received\r\n",
            "debug2: local client KEXINIT proposal\r\n",
            "debug2: KEX algorithms: curve25519-sha256,curve25519-sha256@libssh.org,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256,diffie-hellman-group16-sha512,diffie-hellman-group18-sha512,diffie-hellman-group-exchange-sha1,diffie-hellman-group14-sha256,diffie-hellman-group14-sha1,ext-info-c\r\n",
            "debug2: host key algorithms: ecdsa-sha2-nistp256-cert-v01@openssh.com,ecdsa-sha2-nistp384-cert-v01@openssh.com,ecdsa-sha2-nistp521-cert-v01@openssh.com,ssh-ed25519-cert-v01@openssh.com,ssh-rsa-cert-v01@openssh.com,ecdsa-sha2-nistp256,ecdsa-sha2-nistp384,ecdsa-sha2-nistp521,ssh-ed25519,rsa-sha2-512,rsa-sha2-256,ssh-rsa\r\n",
            "debug2: ciphers ctos: chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com\r\n",
            "debug2: ciphers stoc: chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com\r\n",
            "debug2: MACs ctos: umac-64-etm@openssh.com,umac-128-etm@openssh.com,hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha1-etm@openssh.com,umac-64@openssh.com,umac-128@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-sha1\r\n",
            "debug2: MACs stoc: umac-64-etm@openssh.com,umac-128-etm@openssh.com,hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha1-etm@openssh.com,umac-64@openssh.com,umac-128@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-sha1\r\n",
            "debug2: compression ctos: none,zlib@openssh.com,zlib\r\n",
            "debug2: compression stoc: none,zlib@openssh.com,zlib\r\n",
            "debug2: languages ctos: \r\n",
            "debug2: languages stoc: \r\n",
            "debug2: first_kex_follows 0 \r\n",
            "debug2: reserved 0 \r\n",
            "debug2: peer server KEXINIT proposal\r\n",
            "debug2: KEX algorithms: curve25519-sha256,curve25519-sha256@libssh.org,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256\r\n",
            "debug2: host key algorithms: ssh-dss,rsa-sha2-512,rsa-sha2-256,ssh-rsa\r\n",
            "debug2: ciphers ctos: chacha20-poly1305@openssh.com,aes256-gcm@openssh.com,aes128-gcm@openssh.com,aes256-ctr,aes192-ctr,aes128-ctr,aes256-cbc,aes192-cbc,aes128-cbc\r\n",
            "debug2: ciphers stoc: chacha20-poly1305@openssh.com,aes256-gcm@openssh.com,aes128-gcm@openssh.com,aes256-ctr,aes192-ctr,aes128-ctr,aes256-cbc,aes192-cbc,aes128-cbc\r\n",
            "debug2: MACs ctos: hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha1-etm@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-sha1\r\n",
            "debug2: MACs stoc: hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha1-etm@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-sha1\r\n",
            "debug2: compression ctos: none,zlib,zlib@openssh.com\r\n",
            "debug2: compression stoc: none,zlib,zlib@openssh.com\r\n",
            "debug2: languages ctos: \r\n",
            "debug2: languages stoc: \r\n",
            "debug2: first_kex_follows 0 \r\n",
            "debug2: reserved 0 \r\n",
            "debug1: kex: algorithm: curve25519-sha256\r\n",
            "debug1: kex: host key algorithm: rsa-sha2-512\r\n",
            "debug1: kex: server->client cipher: chacha20-poly1305@openssh.com MAC: <implicit> compression: none\r\n",
            "debug1: kex: client->server cipher: chacha20-poly1305@openssh.com MAC: <implicit> compression: none\r\n",
            "debug3: send packet: type 30\r\n",
            "debug1: expecting SSH2_MSG_KEX_ECDH_REPLY\n",
            "debug3: receive packet: type 31\n",
            "debug1: Server host key: ssh-rsa SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8\n",
            "debug1: read_passphrase: can't open /dev/tty: No such device or address\n",
            "Host key verification failed.\n",
            "Reinitialized existing Git repository in /content/.git/\n",
            "fatal: remote origin already exists.\n",
            "On branch master\n",
            "Untracked files:\n",
            "\t\u001b[31m.config/\u001b[m\n",
            "\t\u001b[31mmy_dataset/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present\n",
            "Host key verification failed.\n",
            "fatal: Could not read from remote repository.\n",
            "\n",
            "Please make sure you have the correct access rights\n",
            "and the repository exists.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_L3FDv4Ffpf",
        "colab_type": "code",
        "outputId": "ae6fa471-d7e1-49ac-e830-e8aa500cda79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git -v"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unknown option: -v\n",
            "usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n",
            "           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n",
            "           [-p | --paginate | --no-pager] [--no-replace-objects] [--bare]\n",
            "           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n",
            "           <command> [<args>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keZ8zMSYfQ8C",
        "colab_type": "code",
        "outputId": "d013074c-ef45-435f-8d93-438ebbd7160c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_dataset  my_model_weights_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyT6C7fAR1bs",
        "colab_type": "code",
        "outputId": "8e6ceb4d-3e7f-41c7-ee10-b455ddbbe915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!rm -r my_dataset\n",
        "!git clone https://github.com/TraverseTowner/my_dataset.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'my_dataset'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 135 (delta 2), reused 12 (delta 2), pack-reused 123\u001b[K\n",
            "Receiving objects: 100% (135/135), 264.03 MiB | 9.76 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhgUQvGkaaDn",
        "colab_type": "code",
        "outputId": "5a41ff68-a1b5-4a19-f34e-41c31f130075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# !ls\n",
        "!rm -r my_dataset\n",
        "!rm -r sample_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'my_dataset': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06KPe2XUeR8C",
        "colab_type": "code",
        "outputId": "00578528-57ab-4cd2-fe68-b2bb1bf7e1f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!pip install -q tf-nightly-gpu\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394.5MB 43kB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501kB 45.4MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2MB 31.1MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-E5ENtregFc",
        "colab_type": "code",
        "outputId": "482f44c7-26c2-4ac1-ba88-bab4cf4382fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_dataset  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlGzmw04mIiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}